{
 "cells": [
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3f6d7cea",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9d917",
   "metadata": {},
   "source": [
    "# The cost function is minimized through gradient decent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a099848",
   "metadata": {},
   "source": [
    "The function we will find the gradient for:\n",
    "f(x) = 3*w_0^2 + 4*w_1^2 - 5*w_0 +7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f4fb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function that will return the value of the equation.\n",
    "\n",
    "def F(w):\n",
    "    \n",
    "    return 3*w[0]**2 + 4*w[1]**2 - 5*w[0] +7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90659426",
   "metadata": {},
   "source": [
    "NOTE: Dude to limitations of computer we need to know that they will never reach the absolutely correct minimum value of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd1ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w):\n",
    "    g=[0]*2 # This is how to initialise arrays.\n",
    "    g[0]= 6*w[0]-5\n",
    "    g[1]= 8*w[1]\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72358476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(w_prev, w_new, lr):\n",
    "    \n",
    "    print(w_prev)\n",
    "    print(w_new)\n",
    "    \n",
    "    while True:\n",
    "        w_prev=w_new\n",
    "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
    "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
    "        \n",
    "        print(w_new)\n",
    "        print(F(w_new))\n",
    "        \n",
    "        if((w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6)):\n",
    "            break\n",
    "            print(\"break\")            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f8426b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3bbcc7de3f3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "descent([4,12],[4,12],0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a5bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.setstyle(\"darkgrid\")\n",
      " 1/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      " 1/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      " 1/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      " 1/5: % Tools\n",
      " 1/6: Dataset\n",
      " 1/7: df=pd.read_csv(\"E:\\New journey\\Datasets\\Breast_cancer_data.csv\")\n",
      " 1/8: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      " 1/9: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "1/10: df.show()\n",
      "1/11: df.head()\n",
      "1/12: df.columns\n",
      "1/13: df['diagnosis'].hist()\n",
      "1/14:\n",
      "corr= data.iloc[:,:-1]\n",
      "corr.head()\n",
      "1/15:\n",
      "corr= df.iloc[:,:-1]\n",
      "corr.head()\n",
      "1/16:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "print(corr)\n",
      "1/17:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_pallette(200, 300, 80, 90, center=\"dark\", as_cmap=True)\n",
      "1/18:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 80, 90, center=\"dark\", as_cmap=True)\n",
      "1/19:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 80, 90, center=\"dark\", as_cmap=True)\n",
      "sns.palplt(cmap)\n",
      "1/20:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 80, 90, center=\"dark\", as_cmap=True)\n",
      "sns.palplot(cmap)\n",
      "1/21:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 80, 90, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, sqaure=True, linewidths=.2)\n",
      "1/22:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 80, 90, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "1/23:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "1/24:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(500, 900, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "1/25:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 80, 90, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "1/26:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 20, 30, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "1/27:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 80, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "1/28:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 90, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "1/29:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "1/30: df.columns\n",
      "1/31: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "1/32: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "1/33: df.head()\n",
      "1/34:\n",
      "fig,axes=plt.subplot(3,1, figsize=(20,10), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean radius\", kde=True, color=\"r\")\n",
      "1/35:\n",
      "fig,axes=plt.subplots(3,1, figsize=(20,10), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean radius\", kde=True, color=\"r\")\n",
      "1/36:\n",
      "fig,axes=plt.subplots(3,1, figsize=(20,10), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\", kde=True, color=\"r\")\n",
      "1/37:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,6), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\", kde=True, color=\"r\")\n",
      "1/38:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,6), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\", kde=True, color=\"r\")\n",
      "1/39:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\", kde=True, color=\"r\")\n",
      "1/40:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "1/41:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "df[\"mean_smoothness\"].head()\n",
      "1/42:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "1/43:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True, sgarex=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "1/44:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True, sharex=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "1/45:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "1/46:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\", color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "1/47:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=False color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "1/48:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=False, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "1/49:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "1/50:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=False, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "1/51:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "1/52: Calculate P(Y=y) for all possible y\n",
      "1/53:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      " 2/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      " 2/2:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      " 2/3: df[Y]\n",
      " 2/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      " 2/5: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\"\n",
      " 2/6: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      " 2/7: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      " 2/8:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      " 2/9: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/10: df.head()\n",
      "2/11: df.columns\n",
      "2/12: df['diagnosis'].hist()\n",
      "2/13:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/14: df.columns\n",
      "2/15: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/16:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/17:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/18: df[Y]\n",
      "2/19: df[\"mean_texture\"]\n",
      "2/20: df[\"mean_texture\"].unique()\n",
      "2/21: type(df[\"mean_texture\"].unique())\n",
      "2/22: list(df[\"mean_texture\"].unique())\n",
      "2/23: sorted(list(df[\"mean_texture\"].unique()))\n",
      "2/24:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_value-mean)**2/(2*std**2)))\n",
      "2/25: list(df.columns)[:-1]\n",
      "2/26: list(df.columns)[:]\n",
      "2/27: ad= [1]*5\n",
      "2/28: ad= [1]*5\n",
      "2/29:\n",
      "ad= [1]*5\n",
      "ad\n",
      "2/30:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            post_prob[j]= likelihood[j]*prior[j]\n",
      "            \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/31:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            post_prob[j]= likelihood[j]*prior[j]\n",
      "            \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/32:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(data, test_size=.2, random_state=41)\n",
      "2/33:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "2/34: test\n",
      "2/35: test.iloc[:,:-1]\n",
      "2/36: test.iloc[:,-1]\n",
      "2/37: test.iloc[:,-1].values()\n",
      "2/38: test.iloc[:,-1].values\n",
      "2/39:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,:-1].values\n",
      "2/40: Y_pred= naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/41:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,:-1].values\n",
      "2/42:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,:-1].values\n",
      "2/43: Y_pred= naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/44:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "2/45: Y_pred= naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/46: feat_name\n",
      "2/47: df\n",
      "2/48:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "        \n",
      "        print(type(likelihood))\n",
      "        print(type(prior))\n",
      "        \n",
      "        for j in range(len(labels)):\n",
      "            print(type(likelihood[j]))\n",
      "            print(type(prior[j]))\n",
      "            post_prob[j]= likelihood[j] * prior[j]\n",
      "            \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/49: Y_pred= naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/50:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "        \n",
      "        print(likelihood)\n",
      "        print(prior)\n",
      "        \n",
      "        for j in range(len(labels)):\n",
      "            print(type(likelihood[j]))\n",
      "            print(type(prior[j]))\n",
      "            post_prob[j]= likelihood[j] * prior[j]\n",
      "            \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/51: Y_pred= naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/52:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/53:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "2/54: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/55: df.head()\n",
      "2/56: df.columns\n",
      "2/57: df['diagnosis'].hist()\n",
      "2/58:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/59: df.columns\n",
      "2/60: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/61:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/62:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/63:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/64:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "        \n",
      "        \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]\n",
      "            \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/65:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,:-1].values\n",
      "2/66: Y_pred= naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/67: Y_pred\n",
      "2/68: train\n",
      "2/69:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(Y_test, Y_pred)\n",
      "2/70:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "2/71:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "2/72:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "2/73: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/74: df.head()\n",
      "2/75: df.columns\n",
      "2/76: df['diagnosis'].hist()\n",
      "2/77:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/78: df.columns\n",
      "2/79: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/80:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/81:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/82:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/83:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "        \n",
      "        \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]\n",
      "            \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/84:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,:-1].values\n",
      "2/85: Y_pred= naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/86:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "2/87: Y_test\n",
      "2/88: Y_pred\n",
      "2/89:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "2/90: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/91: df.head()\n",
      "2/92: df.columns\n",
      "2/93: df['diagnosis'].hist()\n",
      "2/94:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/95: df.columns\n",
      "2/96: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/97:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/98:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/99:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/100:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "        \n",
      "        \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]\n",
      "            \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/101: Y_pred= naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/102: Y_pred\n",
      "2/103:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "2/104: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/105: df.head()\n",
      "2/106: df.columns\n",
      "2/107: df['diagnosis'].hist()\n",
      "2/108:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/109: df.columns\n",
      "2/110: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/111:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/112:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/113:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/114:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        print(Y_Pred)\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/115:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,:-1].values\n",
      "2/116: Y_pred= naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/117:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "2/118: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/119: df.head()\n",
      "2/120: df.columns\n",
      "2/121: df['diagnosis'].hist()\n",
      "2/122:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/123: df.columns\n",
      "2/124: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/125:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/126:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/127:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/128:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        print(Y_Pred)\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/129:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,:-1].values\n",
      "2/130:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "2/131: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/132: df.head()\n",
      "2/133: df.columns\n",
      "2/134: df['diagnosis'].hist()\n",
      "2/135:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/136: df.columns\n",
      "2/137: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/138:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/139:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/140:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/141:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        print(Y_Pred)\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/142:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,:-1].values\n",
      "2/143: Y_pred= naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/144:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "2/145: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/146: df.head()\n",
      "2/147: df.columns\n",
      "2/148: df['diagnosis'].hist()\n",
      "2/149:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/150: df.columns\n",
      "2/151: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/152:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/153:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/154:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/155:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        print(Y_pred)\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/156:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,:-1].values\n",
      "2/157: Y_pred= naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/158: naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/159:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "2/160:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "2/161: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/162: df.head()\n",
      "2/163: df.columns\n",
      "2/164: df['diagnosis'].hist()\n",
      "2/165:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/166: df.columns\n",
      "2/167: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/168:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/169:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/170:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/171:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        print(Y_pred)\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/172:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "2/173: naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/174: Y_pred\n",
      "2/175:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "2/176: Y_test\n",
      "2/177: X_test\n",
      "2/178:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        print(x)\n",
      "        print(Y_pred)\n",
      "        \n",
      "        return(np.array(Y_pred))\n",
      "2/179: naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/180:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        print(x)\n",
      "        print(Y_pred)\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "2/181: Y_pred = naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/182:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "2/183:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "2/184:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "2/185: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/186: df.head()\n",
      "2/187: df.columns\n",
      "2/188: df['diagnosis'].hist()\n",
      "2/189:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/190: df.columns\n",
      "2/191: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/192:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/193:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/194:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/195:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        print(x)\n",
      "        print(Y_pred)\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "2/196:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "2/197: Y_pred = naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/198:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "2/199:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "2/200:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "2/201: df[\"cat_mean_radius\"]=pd.cut(df[\"mean_radius\"].values, bins=3, labels=[0, 1, 2])\n",
      "2/202: df[\"cat_mean_radius\"]=pd.cut(df[\"mean_radius\"].values, bins=3, labels=[0, 1, 2])\n",
      "2/203: df\n",
      "2/204: pd.cut(df[\"mean_radius\"].values, bins=3)\n",
      "2/205:\n",
      "#Binning bing done here\n",
      "df[\"cat_mean_radius\"]=pd.cut(df[\"mean_radius\"].values, bins=3, labels=[0, 1, 2])\n",
      "df[\"cat_mean_texture\"]=pd.cut(df[\"mean_texture\"].values, bins=3, labels=[0, 1, 2])\n",
      "df[\"cat_mean_smoothness\"]=pd.cut(df[\"mean_smoothness\"].values, bins=3, labels=[0, 1, 2])\n",
      "2/206: df.head(1)\n",
      "2/207: df.columns\n",
      "2/208: df=df[['cat_mean_radius', 'cat_mean_texture', 'cat_mean_smoothness', 'diagnosis',]]\n",
      "2/209: df\n",
      "2/210: df.count()\n",
      "2/211: count(df)\n",
      "2/212:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "2/213: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/214: df.head()\n",
      "2/215: df.columns\n",
      "2/216: df['diagnosis'].hist()\n",
      "2/217:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/218: df.columns\n",
      "2/219: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/220:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/221:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/222:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/223:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "2/224:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "2/225: Y_pred = naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/226:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "2/227:\n",
      "#Binning bing done here\n",
      "df[\"cat_mean_radius\"]=pd.cut(df[\"mean_radius\"].values, bins=3, labels=[0, 1, 2])\n",
      "df[\"cat_mean_texture\"]=pd.cut(df[\"mean_texture\"].values, bins=3, labels=[0, 1, 2])\n",
      "df[\"cat_mean_smoothness\"]=pd.cut(df[\"mean_smoothness\"].values, bins=3, labels=[0, 1, 2])\n",
      "2/228: df.columns\n",
      "2/229: df.count()\n",
      "2/230: df.drop('mean_radius', 'mean_texture', 'mean_smoothness')\n",
      "2/231: df.drop(columns=['mean_radius', 'mean_texture', 'mean_smoothness'])\n",
      "2/232: df\n",
      "2/233: df1=df.drop(columns=['mean_radius', 'mean_texture', 'mean_smoothness'])\n",
      "2/234: df1.count()\n",
      "2/235: df1\n",
      "2/236:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "2/237: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "2/238: df.head()\n",
      "2/239: df.columns\n",
      "2/240: df['diagnosis'].hist()\n",
      "2/241:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "2/242: df.columns\n",
      "2/243: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "2/244:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "2/245:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "2/246:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "2/247:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "2/248:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "2/249: Y_pred = naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "2/250:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "2/251:\n",
      "#Binning bing done here\n",
      "df[\"cat_mean_radius\"]=pd.cut(df[\"mean_radius\"].values, bins=3, labels=[0, 1, 2])\n",
      "df[\"cat_mean_texture\"]=pd.cut(df[\"mean_texture\"].values, bins=3, labels=[0, 1, 2])\n",
      "df[\"cat_mean_smoothness\"]=pd.cut(df[\"mean_smoothness\"].values, bins=3, labels=[0, 1, 2])\n",
      "2/252: df.columns\n",
      "2/253: df=df[['cat_mean_radius', 'cat_mean_texture', 'cat_mean_smoothness', 'diagnosis',]]\n",
      "2/254:\n",
      "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
      "    feat=list(df.columns)\n",
      "    df = df[df[Y]==label]\n",
      "    p_x_given_y = len(df[df[feat_name]==feat_val])/len(df)\n",
      "    \n",
      "    return p_x_given_y\n",
      "2/255:\n",
      "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
      "    feat=list(df.columns)\n",
      "    df = df[df[Y]==label]\n",
      "    p_x_given_y = len(df[df[feat_name]==feat_val])/len(df)\n",
      "    \n",
      "    return p_x_given_y\n",
      " 3/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      " 3/2: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      " 3/3: df.head()\n",
      " 3/4: df.columns\n",
      " 3/5: df['diagnosis'].hist()\n",
      " 3/6:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      " 3/7: df.columns\n",
      " 3/8: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      " 3/9:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "3/10:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "3/11:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "3/12:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "3/13:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "3/14: Y_pred = naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "3/15:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "3/16:\n",
      "#Binning bing done here\n",
      "df[\"cat_mean_radius\"]=pd.cut(df[\"mean_radius\"].values, bins=3, labels=[0, 1, 2])\n",
      "df[\"cat_mean_texture\"]=pd.cut(df[\"mean_texture\"].values, bins=3, labels=[0, 1, 2])\n",
      "df[\"cat_mean_smoothness\"]=pd.cut(df[\"mean_smoothness\"].values, bins=3, labels=[0, 1, 2])\n",
      "3/17: df=df[['cat_mean_radius', 'cat_mean_texture', 'cat_mean_smoothness', 'diagnosis',]]\n",
      "3/18:\n",
      "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
      "    feat=list(df.columns)\n",
      "    df = df[df[Y]==label]\n",
      "    p_x_given_y = len(df[df[feat_name]==feat_val])/len(df)\n",
      "    \n",
      "    return p_x_given_y\n",
      "3/19:\n",
      "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    p_x_given_Y= len(df[df[feat_name]==feat_val])/len(df)\n",
      "    return p_x_given_y\n",
      "3/20:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                print(likelihood[j])\n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "3/21: Y_pred = naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "3/22:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                print(likelihood[j],\"\\n Change \")\n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "3/23: Y_pred = naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "3/24:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j]* = calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                print(likelihood[j],\"\\n Change \")\n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "3/25:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] *= calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])\n",
      "                print(likelihood[j],\"\\n Change \")\n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "3/26:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "3/27: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "3/28: df.head()\n",
      "3/29: df.columns\n",
      "3/30: df['diagnosis'].hist()\n",
      "3/31:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "3/32: df.columns\n",
      "3/33: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "3/34:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "3/35:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "3/36:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "3/37:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] *= calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "3/38:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "3/39: Y_pred = naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "3/40:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "3/41:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "3/42: df=pd.read_csv(\"E:\\\\New journey\\\\Datasets\\\\Breast_cancer_data.csv\")\n",
      "3/43: df.head()\n",
      "3/44: df.columns\n",
      "3/45: df['diagnosis'].hist()\n",
      "3/46:\n",
      "corr= df.iloc[:,:-1].corr(method=\"pearson\")\n",
      "cmap = sns.diverging_palette(200, 300, 50, 60, center=\"dark\", as_cmap=True)\n",
      "sns.heatmap(corr, vmax=1, vmin=-.5, cmap=cmap, square=True, linewidths=.2)\n",
      "3/47: df.columns\n",
      "3/48: df=df[['mean_radius', 'mean_texture','mean_smoothness', 'diagnosis']]\n",
      "3/49:\n",
      "fig,axes=plt.subplots(3,1, figsize=(18,15), sharey=True)\n",
      "sns.histplot(df, ax=axes[0], x=\"mean_radius\",kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[1], x=\"mean_texture\", kde=True, color=\"r\")\n",
      "sns.histplot(df, ax=axes[2], x=\"mean_smoothness\", kde=True, color=\"r\")\n",
      "3/50:\n",
      "def calculate_prior(df, Y):\n",
      "    classes= sorted(list(df[Y].unique()))\n",
      "    prior=[]\n",
      "    for i in classes:\n",
      "        prior.append(len(df[df==i])/len(df))\n",
      "    return prior\n",
      "3/51:\n",
      "def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    mean, std = df[feat_name].mean(), df[feat_name].std()\n",
      "    p_x_given_y = (1/(np.sqrt(2*np.pi)*std)) * np.exp(-((feat_val-mean)**2/(2*std**2)))\n",
      "    return p_x_given_y\n",
      "3/52:\n",
      "def naive_bayes_gaussian(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j] *= calculate_likelihood_gaussian(df, features[i], x[i], Y, labels[j])                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "3/53:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "3/54: Y_pred = naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "3/55:\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "3/56:\n",
      "#Binning bing done here\n",
      "df[\"cat_mean_radius\"]=pd.cut(df[\"mean_radius\"].values, bins=3, labels=[0, 1, 2])\n",
      "df[\"cat_mean_texture\"]=pd.cut(df[\"mean_texture\"].values, bins=3, labels=[0, 1, 2])\n",
      "df[\"cat_mean_smoothness\"]=pd.cut(df[\"mean_smoothness\"].values, bins=3, labels=[0, 1, 2])\n",
      "3/57: df=df[['cat_mean_radius', 'cat_mean_texture', 'cat_mean_smoothness', 'diagnosis',]]\n",
      "3/58:\n",
      "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
      "    feat=list(df.columns)\n",
      "    df = df[df[Y]==label]\n",
      "    p_x_given_y = len(df[df[feat_name]==feat_val])/len(df)\n",
      "    \n",
      "    return p_x_given_y\n",
      "3/59:\n",
      "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    p_x_given_Y= len(df[df[feat_name]==feat_val])/len(df)\n",
      "    return p_x_given_y\n",
      "3/60:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "\n",
      "Y_pred = naive_bayes_gaussian(train, X=X_test, Y=\"diagnosis\")\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "3/61:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "\n",
      "Y_pred = naive_bayes_categorical(train, X=X_test, Y=\"diagnosis\")\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "3/62:\n",
      "def naive_bayes_categorical(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j]*= calculate_likelihood_categorical(df, features[i], x[i], Y, labels[j])\n",
      "                print(likelihood[j],\"\\n Change \")\n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "3/63:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "\n",
      "Y_pred = naive_bayes_categorical(train, X=X_test, Y=\"diagnosis\")\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "3/64:\n",
      "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
      "    feat= list(df.columns)\n",
      "    df= df[df[Y]==label]\n",
      "    p_x_given_y= len(df[df[feat_name]==feat_val])/len(df)\n",
      "    return p_x_given_y\n",
      "3/65:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "\n",
      "Y_pred = naive_bayes_categorical(train, X=X_test, Y=\"diagnosis\")\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      "3/66:\n",
      "def naive_bayes_categorical(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j]*= calculate_likelihood_categorical(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "3/67:\n",
      "def naive_bayes_categorical(df, X, Y):\n",
      "    \n",
      "    # get features name\n",
      "    features= list(df.columns)[:-1]\n",
      "    \n",
      "    # calculate prior\n",
      "    prior= calculate_prior(df, Y)\n",
      "    \n",
      "    Y_pred=[]\n",
      "    #loop over every data smaple\n",
      "    for x in X:\n",
      "        #calculate likelihood\n",
      "        labels= sorted(list(df[Y].unique()))\n",
      "        likelihood=[1]*len(labels)\n",
      "        for j in range(len(labels)):\n",
      "            for i in range(len(features)):\n",
      "                likelihood[j]*= calculate_likelihood_categorical(df, features[i], x[i], Y, labels[j])\n",
      "                \n",
      "        #Calculate posterior probability\n",
      "        post_prob =[1]*len(labels)\n",
      "                \n",
      "        for j in range(len(labels)):        \n",
      "            post_prob[j]= likelihood[j] * prior[j]            \n",
      "                \n",
      "        Y_pred.append(np.argmax(post_prob))\n",
      "        \n",
      "    return(np.array(Y_pred))\n",
      "3/68:\n",
      "from sklearn.model_selection import train_test_split\n",
      "train, test = train_test_split(df, test_size=.2, random_state=41)\n",
      "\n",
      "X_test = test.iloc[:,:-1].values\n",
      "Y_test = test.iloc[:,-1].values\n",
      "\n",
      "Y_pred = naive_bayes_categorical(train, X=X_test, Y=\"diagnosis\")\n",
      "from sklearn.metrics import confusion_matrix, f1_score\n",
      "print(confusion_matrix(Y_test, Y_pred))\n",
      "print(f1_score(Y_test, Y_pred))\n",
      " 5/1:\n",
      "class KNN:\n",
      "    \n",
      "    #To connect object instances their attribute.\n",
      "    def _init_(self, k=3):\n",
      "        self.k=k\n",
      "\n",
      "    #Will fit the trainig samples and training methods.    \n",
      "    def fit(self, X, y):\n",
      "        pass\n",
      "    \n",
      "    def predict(self, X):\n",
      "        pass\n",
      " 5/2: import numpy as np\n",
      " 5/3:\n",
      "def predict_each(self, x):\n",
      "    pass\n",
      " 5/4:\n",
      "class KNN:\n",
      "    \n",
      "    #To connect object instances their attribute.\n",
      "    def _init_(self, k=3):\n",
      "        self.k=k\n",
      "\n",
      "    #Will fit the trainig samples and training methods.\n",
      "    #Since there is no training involved in KNN. We will just assign the features and labels here.\n",
      "    def fit(self, X, y):\n",
      "        self.X_train = X\n",
      "        self.Y_train = y\n",
      "    \n",
      "    '''This method will be passed the complete collection of samples. To keep the structure modular, we'll \n",
      "    create another function to predict each example/sample in the collection'''\n",
      "    def predict(self, X):\n",
      "        predicted_labels = [self.predict_each(x) for x in X]\n",
      "        return np.array(predicted_labels)\n",
      " 6/1:\n",
      "'''This method will be passed the complete collection of samples. To keep the structure modular, we'll \n",
      "    create another function to predict each example/sample in the collection'''\n",
      "    def predict(self, X):\n",
      "        predicted_labels = [self.predict_each(x) for x in X]\n",
      "        return np.array(predicted_labels)\n",
      " 6/2:\n",
      "'''This method will be passed the complete collection of samples. To keep the structure modular, we'll \n",
      "    create another function to predict each example/sample in the collection'''\n",
      "def predict(self, X):\n",
      "    predicted_labels = [self.predict_each(x) for x in X]\n",
      "    return np.array(predicted_labels)\n",
      " 6/3: import numpy as np\n",
      " 6/4:\n",
      "class KNN:\n",
      "    \n",
      "    #To connect object instances their attribute.\n",
      "    def _init_(self, k=3):\n",
      "        self.k=k\n",
      "\n",
      "    #Will fit the trainig samples and training methods.\n",
      "    #Since there is no training involved in KNN. We will just assign the features and labels here.\n",
      "    def fit(self, X, y):\n",
      "        self.X_train = X\n",
      "        self.Y_train = y\n",
      " 6/5:\n",
      "'''This method will be passed the complete collection of samples. To keep the structure modular, we'll \n",
      "    create another function to predict each example/sample in the collection'''\n",
      "def predict(self, X):\n",
      "    predicted_labels = [self.predict_each(x) for x in X]\n",
      "    return np.array(predicted_labels)\n",
      " 6/6:\n",
      "def predict_each(self, x):\n",
      "    pass\n",
      " 6/7:\n",
      "def euclidean_distance(x1, x2):\n",
      "    return np.sqrt(np.sum((x1-x2)**2))\n",
      " 6/8:\n",
      "a=[3,8,1,76,12,54,1,3,]\n",
      "print(np.argsort(a))\n",
      " 6/9:\n",
      "class KNN:\n",
      "    \n",
      "    #To connect object instances their attribute.\n",
      "    def _init_(self, k=3):\n",
      "        self.k=k\n",
      "\n",
      "    #Will fit the trainig samples and training methods.\n",
      "    #Since there is no training involved in KNN. We will just assign the features and labels here.\n",
      "    def fit(self, X, y):\n",
      "        self.X_train = X\n",
      "        self.y_train = y\n",
      "6/10:\n",
      "import numpy as np\n",
      "from collections import counter\n",
      "6/11:\n",
      "import numpy as np\n",
      "from Collections import counter\n",
      "6/12:\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "6/13:\n",
      "def predict_each(self, x):\n",
      "    #compute distances\n",
      "    distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
      "    \n",
      "    #get k nearest samples, labels\n",
      "    k_indices = np.argsort(distances)[:self.k]\n",
      "    k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
      "    \n",
      "    #majority vote, most common class label\n",
      "    most_common = Counter(k_nearest_labels).most_common(1)\n",
      "    \n",
      "    return most_common[0][0]\n",
      " 7/1: from 1st KNN import KNN\n",
      " 7/2: from KNN_scratch import KNN\n",
      "6/14:\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "6/15:\n",
      "def euclidean_distance(x1, x2):\n",
      "    return np.sqrt(np.sum((x1-x2)**2))\n",
      "6/16:\n",
      "class KNN:\n",
      "    \n",
      "    #To connect object instances their attribute.\n",
      "    def _init_(self, k=3):\n",
      "        self.k=k\n",
      "\n",
      "    #Will fit the trainig samples and training methods.\n",
      "    #Since there is no training involved in KNN. We will just assign the features and labels here.\n",
      "    def fit(self, X, y):\n",
      "        self.X_train = X\n",
      "        self.y_train = y\n",
      "6/17:\n",
      "'''This method will be passed the complete collection of samples. To keep the structure modular, we'll \n",
      "    create another function to predict each example/sample in the collection'''\n",
      "def predict(self, X):\n",
      "    predicted_labels = [self.predict_each(x) for x in X]\n",
      "    return np.array(predicted_labels)\n",
      "6/18:\n",
      "def predict_each(self, x):\n",
      "    #compute distances\n",
      "    distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
      "    \n",
      "    #get k nearest samples, labels\n",
      "    k_indices = np.argsort(distances)[:self.k]\n",
      "    k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
      "    \n",
      "    #majority vote, most common class label\n",
      "    most_common = Counter(k_nearest_labels).most_common(1)\n",
      "    \n",
      "    return most_common[0][0]\n",
      " 8/1: from ipynb.fs.full.KNN_scratch import KNN\n",
      "10/1:\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "10/2:\n",
      "def euclidean_distance(x1, x2):\n",
      "    return np.sqrt(np.sum((x1-x2)**2))\n",
      "10/3:\n",
      "class KNN:\n",
      "    \n",
      "    #To connect object instances their attribute.\n",
      "    def _init_(self, k=3):\n",
      "        self.k=k\n",
      "\n",
      "    #Will fit the trainig samples and training methods.\n",
      "    #Since there is no training involved in KNN. We will just assign the features and labels here.\n",
      "    def fit(self, X, y):\n",
      "        self.X_train = X\n",
      "        self.y_train = y\n",
      "10/4:\n",
      "'''This method will be passed the complete collection of samples. To keep the structure modular, we'll \n",
      "    create another function to predict each example/sample in the collection'''\n",
      "def predict(self, X):\n",
      "    predicted_labels = [self.predict_each(x) for x in X]\n",
      "    return np.array(predicted_labels)\n",
      "10/5:\n",
      "def predict_each(self, x):\n",
      "    #compute distances\n",
      "    distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
      "    \n",
      "    #get k nearest samples, labels\n",
      "    k_indices = np.argsort(distances)[:self.k]\n",
      "    k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
      "    \n",
      "    #majority vote, most common class label\n",
      "    most_common = Counter(k_nearest_labels).most_common(1)\n",
      "    \n",
      "    return most_common[0][0]\n",
      " 9/1: from ipynb.fs.full.KNN_scratch import KNN\n",
      " 9/2: clf = KNN(k=3)\n",
      " 9/3: KNN\n",
      "10/6: KNN\n",
      "10/7: KNN.predict(45)\n",
      "10/8: KNN.fit(45)\n",
      "10/9:\n",
      "'''This method will be passed the complete collection of samples. To keep the structure modular, we'll \n",
      "    create another function to predict each example/sample in the collection'''\n",
      "    def predict(self, X):\n",
      "        predicted_labels = [self.predict_each(x) for x in X]\n",
      "        return np.array(predicted_labels)\n",
      "10/10:\n",
      "class KNN:\n",
      "    \n",
      "    #To connect object instances their attribute.\n",
      "    def _init_(self, k=3):\n",
      "        self.k=k\n",
      "\n",
      "    #Will fit the trainig samples and training methods.\n",
      "    #Since there is no training involved in KNN. We will just assign the features and labels here.\n",
      "    def fit(self, X, y):\n",
      "        self.X_train = X\n",
      "        self.y_train = y\n",
      "    \n",
      "    '''This method will be passed the complete collection of samples. To keep the structure modular, we'll \n",
      "    create another function to predict each example/sample in the collection'''\n",
      "    def predict(self, X):\n",
      "        predicted_labels = [self.predict_each(x) for x in X]\n",
      "        return np.array(predicted_labels)\n",
      "    \n",
      "    def predict_each(self, x):\n",
      "        #compute distances\n",
      "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
      "    \n",
      "        #get k nearest samples, labels\n",
      "        k_indices = np.argsort(distances)[:self.k]\n",
      "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
      "    \n",
      "        #majority vote, most common class label\n",
      "        most_common = Counter(k_nearest_labels).most_common(1)\n",
      "    \n",
      "        return most_common[0][0]\n",
      "11/1:\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "11/2:\n",
      "def euclidean_distance(x1, x2):\n",
      "    return np.sqrt(np.sum((x1-x2)**2))\n",
      "11/3:\n",
      "class KNN:\n",
      "    \n",
      "    #To connect object instances their attribute.\n",
      "    def _init_(self, k=3):\n",
      "        self.k=k\n",
      "\n",
      "    #Will fit the trainig samples and training methods.\n",
      "    #Since there is no training involved in KNN. We will just assign the features and labels here.\n",
      "    def fit(self, X, y):\n",
      "        self.X_train = X\n",
      "        self.y_train = y\n",
      "    \n",
      "    '''This method will be passed the complete collection of samples. To keep the structure modular, we'll \n",
      "    create another function to predict each example/sample in the collection'''\n",
      "    def predict(self, X):\n",
      "        predicted_labels = [self.predict_each(x) for x in X]\n",
      "        return np.array(predicted_labels)\n",
      "    \n",
      "    def predict_each(self, x):\n",
      "        #compute distances\n",
      "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
      "    \n",
      "        #get k nearest samples, labels\n",
      "        k_indices = np.argsort(distances)[:self.k]\n",
      "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
      "    \n",
      "        #majority vote, most common class label\n",
      "        most_common = Counter(k_nearest_labels).most_common(1)\n",
      "    \n",
      "        return most_common[0][0]\n",
      " 9/4: from ipynb.fs.full.KNN_scratch import KNN\n",
      " 9/5: from ipynb.fs.full.KNN_scratch import KNN\n",
      " 9/6: KNN\n",
      " 9/7: KNN(k=3)\n",
      "11/4: KNN(k=4)\n",
      "11/5:\n",
      "class KNN:\n",
      "    \n",
      "    #To connect object instances their attribute.\n",
      "    def __init__(self, k=3):\n",
      "        self.k=k\n",
      "\n",
      "    #Will fit the trainig samples and training methods.\n",
      "    #Since there is no training involved in KNN. We will just assign the features and labels here.\n",
      "    def fit(self, X, y):\n",
      "        self.X_train = X\n",
      "        self.y_train = y\n",
      "    \n",
      "    '''This method will be passed the complete collection of samples. To keep the structure modular, we'll \n",
      "    create another function to predict each example/sample in the collection'''\n",
      "    def predict(self, X):\n",
      "        predicted_labels = [self.predict_each(x) for x in X]\n",
      "        return np.array(predicted_labels)\n",
      "    \n",
      "    def predict_each(self, x):\n",
      "        #compute distances\n",
      "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
      "    \n",
      "        #get k nearest samples, labels\n",
      "        k_indices = np.argsort(distances)[:self.k]\n",
      "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
      "    \n",
      "        #majority vote, most common class label\n",
      "        most_common = Counter(k_nearest_labels).most_common(1)\n",
      "    \n",
      "        return most_common[0][0]\n",
      "11/6: KNN(k=4)\n",
      " 9/8: clf=KNN(k=6)\n",
      " 9/9: from ipynb.fs.full.KNN_scratch import KNN\n",
      "9/10: clf=KNN(k=6)\n",
      "9/11: from ipynb.fs.full.KNN_scratch import KNN\n",
      "9/12: clf=KNN(k=6)\n",
      "12/1:\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "12/2:\n",
      "def euclidean_distance(x1, x2):\n",
      "    return np.sqrt(np.sum((x1-x2)**2))\n",
      "12/3:\n",
      "class KNN:\n",
      "    \n",
      "    #To connect object instances their attribute.\n",
      "    def __init__(self, k=3):\n",
      "        self.k=k\n",
      "\n",
      "    #Will fit the trainig samples and training methods.\n",
      "    #Since there is no training involved in KNN. We will just assign the features and labels here.\n",
      "    def fit(self, X, y):\n",
      "        self.X_train = X\n",
      "        self.y_train = y\n",
      "    \n",
      "    '''This method will be passed the complete collection of samples. To keep the structure modular, we'll \n",
      "    create another function to predict each example/sample in the collection'''\n",
      "    def predict(self, X):\n",
      "        predicted_labels = [self.predict_each(x) for x in X]\n",
      "        return np.array(predicted_labels)\n",
      "    \n",
      "    def predict_each(self, x):\n",
      "        #compute distances\n",
      "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
      "    \n",
      "        #get k nearest samples, labels\n",
      "        k_indices = np.argsort(distances)[:self.k]\n",
      "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
      "    \n",
      "        #majority vote, most common class label\n",
      "        most_common = Counter(k_nearest_labels).most_common(1)\n",
      "    \n",
      "        return most_common[0][0]\n",
      "12/4: KNN(k=4)\n",
      "9/13: from ipynb.fs.full.KNN_scratch import KNN\n",
      "9/14: clf=KNN(k=6)\n",
      "13/1: from ipynb.fs.full.KNN_scratch import KNN\n",
      "13/2: clf=KNN(k=6)\n",
      "13/3:\n",
      "from ipynb.fs.full.KNN_scratch import KNN\n",
      "from sklearn.model_selection import train_test_split\n",
      "13/4:\n",
      "from ipynb.fs.full.KNN_scratch import KNN\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "from sklearn import datasets\n",
      "from sklearn.model_selection import train_test_split\n",
      "13/5: iris = datasets.load_iris()\n",
      "13/6:\n",
      "X, y = iris.data, iris.target\n",
      "print(X, \"\\n\", y)\n",
      "13/7:\n",
      "X, y = iris.data, iris.target\n",
      "print( y)\n",
      "13/8:\n",
      "iris = datasets.load_iris()\n",
      "iris.head(10)\n",
      "13/9:\n",
      "iris = datasets.load_iris()\n",
      "type(iris)\n",
      "13/10: type(X)\n",
      "13/11: length(X)\n",
      "13/12: len(X)\n",
      "13/13: len(y)\n",
      "13/14: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
      "13/15:\n",
      "#Creatd an object of class KNN\n",
      "clf=KNN(k=6)\n",
      "\n",
      "# Calling a fit function from the class KNN and sending data.\n",
      "clf.fit(X_train, y_train)\n",
      "13/16:\n",
      "#Creatd an object of class KNN\n",
      "clf=KNN(k=6)\n",
      "\n",
      "# Calling a fit function from the class KNN and sending data.\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "predictions = clf.predict(X_test)\n",
      "13/17: predictions\n",
      "13/18: X_test\n",
      "13/19: len(X_test)\n",
      "13/20: len(predictions)\n",
      "13/21:\n",
      "#checking accuracy\n",
      "acc= np.sum(predictions== y_test)/len(y_test)\n",
      "13/22:\n",
      "import numpy as np\n",
      "from ipynb.fs.full.KNN_scratch import KNN\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "from sklearn import datasets\n",
      "from sklearn.model_selection import train_test_split\n",
      "13/23:\n",
      "#checking accuracy\n",
      "acc= np.sum(predictions== y_test)/len(y_test)\n",
      "13/24:\n",
      "#checking accuracy\n",
      "acc= np.sum(predictions== y_test)/len(y_test)\n",
      "print(acc)\n",
      "13/25: predictions\n",
      "13/26: y_test\n",
      "13/27:\n",
      "#Creatd an object of class KNN\n",
      "clf=KNN(k=5)\n",
      "\n",
      "# Calling a fit function from the class KNN and sending data.\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "predictions = clf.predict(X_test)\n",
      "13/28:\n",
      "#checking accuracy\n",
      "acc= np.sum(predictions== y_test)/len(y_test)\n",
      "print(acc)\n",
      "19/1:\n",
      "import numpy as np\n",
      "from ipynb.fs.full.KNN_scratch import KNN\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "from sklearn import datasets\n",
      "from sklearn.model_selection import train_test_split\n",
      "19/2: iris = datasets.load_iris()\n",
      "24/1:\n",
      "#The function that will return the value of the equation.\n",
      "\n",
      "def F(w):\n",
      "    \n",
      "    return 3*w[0]**2 + 4*w[1]**2 - 5*w[0] +7\n",
      "24/2:\n",
      "def gradient(w):\n",
      "    g[0]= 6*w[0]-5\n",
      "    g[1]= 8*w[1]\n",
      "    return g\n",
      "24/3:\n",
      "def descent(w_prev, w_new, lr):\n",
      "    \n",
      "    print(w_prev)\n",
      "    print(w_new)\n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "        \n",
      "        print(w_new)\n",
      "        print(F(w_new))\n",
      "        \n",
      "        if(w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6)\n",
      "            break\n",
      "            print(\"break\")\n",
      "24/4:\n",
      "def descent(w_prev, w_new, lr):\n",
      "    \n",
      "    print(w_prev)\n",
      "    print(w_new)\n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "        \n",
      "        print(w_new)\n",
      "        print(F(w_new))\n",
      "        \n",
      "        if((w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2) < pow(10,-6)\n",
      "            break\n",
      "            print(\"break\")\n",
      "24/5:\n",
      "def descent(w_prev, w_new, lr):\n",
      "    \n",
      "    print(w_prev)\n",
      "    print(w_new)\n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "        \n",
      "        print(w_new)\n",
      "        print(F(w_new))\n",
      "        \n",
      "        if(((w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2) < pow(10,-6))\n",
      "            break\n",
      "            print(\"break\")\n",
      "24/6:\n",
      "def descent(w_prev, w_new, lr):\n",
      "    \n",
      "    print(w_prev)\n",
      "    print(w_new)\n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "        \n",
      "        print(w_new)\n",
      "        print(F(w_new))\n",
      "        \n",
      "        if(w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6):\n",
      "            break\n",
      "            print(\"break\")\n",
      "24/7: desceny([4,12],[4,12],0.2)\n",
      "24/8: descent([4,12],[4,12],0.2)\n",
      "24/9:\n",
      "#The function that will return the value of the equation.\n",
      "\n",
      "def F(w):\n",
      "    \n",
      "    return 3*w[0]**2 + 4*w[1]**2 - 5*w[0] +7\n",
      "24/10:\n",
      "def gradient(w):\n",
      "    g[0]= 6*w[0]-5\n",
      "    g[1]= 8*w[1]\n",
      "    return g\n",
      "24/11: descent([4,12],[4,12],0.2)\n",
      "24/12:\n",
      "def gradient(w):\n",
      "    g=g[0]*2\n",
      "    g[0]= 6*w[0]-5\n",
      "    g[1]= 8*w[1]\n",
      "    return g\n",
      "24/13: descent([4,12],[4,12],0.2)\n",
      "25/1:\n",
      "#The function that will return the value of the equation.\n",
      "\n",
      "def F(w):\n",
      "    \n",
      "    return 3*w[0]**2 + 4*w[1]**2 - 5*w[0] +7\n",
      "25/2:\n",
      "def gradient(w):\n",
      "    g=g[0]*2\n",
      "    g[0]= 6*w[0]-5\n",
      "    g[1]= 8*w[1]\n",
      "    return g\n",
      "25/3:\n",
      "def descent(w_prev, w_new, lr):\n",
      "    \n",
      "    print(w_prev)\n",
      "    print(w_new)\n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "        \n",
      "        print(w_new)\n",
      "        print(F(w_new))\n",
      "        \n",
      "        if(w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6):\n",
      "            break\n",
      "            print(\"break\")\n",
      "25/4: descent([4,12],[4,12],0.2)\n",
      "25/5:\n",
      "def gradient(w):\n",
      "    g=[0]*2\n",
      "    g[0]= 6*w[0]-5\n",
      "    g[1]= 8*w[1]\n",
      "    return g\n",
      "25/6:\n",
      "def gradient(w):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= 6*w[0]-5\n",
      "    g[1]= 8*w[1]\n",
      "    return g\n",
      "25/7: descent([4,12],[4,12],0.2)\n",
      "26/1:\n",
      "# generating fake data\n",
      "from sklearn.datasets.samples_generator import make_regression\n",
      "X, y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30, random_state=200)\n",
      "m=200\n",
      "26/2: X\n",
      "28/1:\n",
      "# generating fake data\n",
      "from sklearn.datasets.samples_generator import make_regression\n",
      "X, y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30, random_state=200)\n",
      "m=200\n",
      "29/1:\n",
      "# generating fake data\n",
      "from sklearn.datasets.samples_generator import make_regression\n",
      "X, y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30, random_state=200)\n",
      "m=200\n",
      "29/2:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "29/3:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30, random_state=200)\n",
      "m=200\n",
      "29/4:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1, noise=6, bias=30, random_state=200)\n",
      "m=200\n",
      "29/5:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1,  bias=30, random_state=200)\n",
      "m=200\n",
      "29/6:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1,   random_state=200)\n",
      "m=200\n",
      "29/7: X\n",
      "29/8: import matplotlib.pyplot as pl\n",
      "29/9: import matplotlib.pyplot as plt\n",
      "29/10:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"0\")\n",
      "29/11:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "29/12:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1, shuffle=True,  random_state=200)\n",
      "m=200\n",
      "29/13:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/1:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1, shuffle=False,  random_state=200)\n",
      "m=200\n",
      "30/2:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/3:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1, shuffle=False,  random_state=200, centeres=2)\n",
      "m=200\n",
      "30/4:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1, shuffle=False,  random_state=200, centere=2)\n",
      "m=200\n",
      "30/5:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1, shuffle=False,  random_state=200, center=2)\n",
      "m=200\n",
      "30/6:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1, shuffle=False,  random_state=200, centers=2)\n",
      "m=200\n",
      "30/7:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/8:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1, shuffle=False,  random_state=200, centers=10)\n",
      "m=200\n",
      "30/9:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/10:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1, shuffle=False,  random_state=200)\n",
      "m=200\n",
      "30/11:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/12:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1,  random_state=200)\n",
      "m=200\n",
      "30/13:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/14: y\n",
      "30/15: X,y\n",
      "30/16: X1y1=make_regression(n_samples=400, n_features=1,  random_state=200)\n",
      "30/17:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_blobs\n",
      "X, y= make_blobs(n_samples=400, n_features=1,  random_state=200)\n",
      "m=200\n",
      "30/18: X1y1=make_regression(n_samples=400, n_features=1,  random_state=200)\n",
      "30/19: X1,1=make_regression(n_samples=400, n_features=1,  random_state=200)\n",
      "30/20: X1,y1= make_regression(n_samples=400, n_features=1,  random_state=200)\n",
      "30/21:\n",
      "from sklearn.datasets.samples_generator import make_regression\n",
      "#X1,y1= make_regression(n_samples=400, n_features=1,  random_state=200)\n",
      "30/22:\n",
      "from sklearn.datasets import make_blobs, make_regression\n",
      "#X1,y1= make_regression(n_samples=400, n_features=1,  random_state=200)\n",
      "30/23:\n",
      "from sklearn.datasets import make_blobs, make_regression\n",
      "X1,y1= make_regression(n_samples=400, n_features=1,  random_state=200)\n",
      "30/24:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "30/25:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/26:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=20, bias=30)\n",
      "m=200\n",
      "30/27:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/28:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "30/29:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/30:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=10)\n",
      "m=200\n",
      "30/31:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/32:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=1000)\n",
      "m=200\n",
      "30/33:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/34:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=0)\n",
      "m=200\n",
      "30/35:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/36:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=2000)\n",
      "m=200\n",
      "30/37:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/38:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "30/39:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "30/40:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "plt.xlabel(\"X\")\n",
      "plt.ylabel(\"y\")\n",
      "plt.show()\n",
      "30/41: type(X)\n",
      "30/42: type([0]*2)\n",
      "30/43: X\n",
      "30/44:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w, X):\n",
      "    return w[1](np.array(X[]))+w[0]\n",
      "30/45:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w, X):\n",
      "    return w[1](np.array(X[:,0]))+w[0]\n",
      "30/46: a=np.array(2,4,5,6,7,8,9)\n",
      "30/47: a=np.array([2,4,5,6,7,8,9])\n",
      "30/48:\n",
      "a=np.array([2,4,5,6,7,8,9])\n",
      "b=np.array([6,9,1,11,7,14,5])\n",
      "30/49: a-b\n",
      "30/50: np.sum(a,b)\n",
      "30/51: np.sum(a)\n",
      "30/52: np.sum(a-b)\n",
      "30/53: np.square(a-b)\n",
      "30/54:\n",
      "def cost(w, X, y):\n",
      "    return np.sum(np.square(LM(w,X)-y)) / (2m)\n",
      "30/55: m\n",
      "30/56:\n",
      "def cost(w, X, y):\n",
      "    return np.sum(np.square(LM(w,X)-y)) / (2*m)\n",
      "30/57: np.multiply(a,b)\n",
      "30/58:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w,X)-y)/m\n",
      "    g[1]= np.multiply(np.sum(LM(w,X)-y),X)/m\n",
      "    return g\n",
      "30/59:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "    \n",
      "    print(w_prev)\n",
      "    print(w_new)\n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "        \n",
      "        print(w_new)\n",
      "        print(F(w_new))\n",
      "        \n",
      "        if(w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6):\n",
      "            break\n",
      "            print(\"break\")\n",
      "30/60: descent([5,10],[5,10],.3, X,y)\n",
      "30/61:\n",
      "def cost(w, X, y):\n",
      "    return (np.sum(np.square(LM(w,X)-y)) / (2*m))\n",
      "30/62:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w, X):\n",
      "    return (w[1](np.array(X[:,0]))+w[0])\n",
      "30/63: descent([5,10],[5,10],.3, X,y)\n",
      "30/64:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w, X):\n",
      "    return (w[1]*(np.array(X[:,0]))+w[0])\n",
      "30/65: descent([5,10],[5,10],.3, X,y)\n",
      "30/66:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "    \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "        \n",
      "        print(w_new)\n",
      "        print(F(w_new))\n",
      "        \n",
      "        if(w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6):\n",
      "            break\n",
      "            print(\"break\")\n",
      "30/67:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "    \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        \n",
      "        if(w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/68: descent([5,10],[5,10],.3, X,y)\n",
      "31/1:\n",
      "def descent(w_prev, w_new, lr):\n",
      "    \n",
      "    print(w_prev)\n",
      "    print(w_new)\n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "        \n",
      "        print(w_new)\n",
      "        print(F(w_new))\n",
      "        \n",
      "        if((w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6):\n",
      "            break\n",
      "            print(\"break\")\n",
      "31/2:\n",
      "def descent(w_prev, w_new, lr):\n",
      "    \n",
      "    print(w_prev)\n",
      "    print(w_new)\n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "        \n",
      "        print(w_new)\n",
      "        print(F(w_new))\n",
      "        \n",
      "        if(w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6):\n",
      "            break\n",
      "            print(\"break\")\n",
      "31/3:\n",
      "def descent(w_prev, w_new, lr):\n",
      "    \n",
      "    print(w_prev)\n",
      "    print(w_new)\n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "        \n",
      "        print(w_new)\n",
      "        print(F(w_new))\n",
      "        \n",
      "        if((w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "31/4: descent([4,12],[4,12],0.2)\n",
      "31/5:\n",
      "#The function that will return the value of the equation.\n",
      "\n",
      "def F(w):\n",
      "    \n",
      "    return 3*w[0]**2 + 4*w[1]**2 - 5*w[0] +7\n",
      "31/6:\n",
      "def gradient(w):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= 6*w[0]-5\n",
      "    g[1]= 8*w[1]\n",
      "    return g\n",
      "31/7:\n",
      "def descent(w_prev, w_new, lr):\n",
      "    \n",
      "    print(w_prev)\n",
      "    print(w_new)\n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "        \n",
      "        print(w_new)\n",
      "        print(F(w_new))\n",
      "        \n",
      "        if((w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "31/8: descent([4,12],[4,12],0.2)\n",
      "30/69:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "    \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        \n",
      "        if((w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/70:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        \n",
      "        if((w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/71: descent([5,10],[5,10],.3, X,y)\n",
      "30/72: w_new\n",
      "30/73:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(w_new)\n",
      "        if((w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/74: descent([5,10],[5,10],.3, X,y)\n",
      "30/75: a\n",
      "30/76: a[0]\n",
      "30/77:\n",
      "if((a[0]-b[0])**2 +(a[1]-b[1])**2 < pow(10,-6)):\n",
      "    print(\"Yes\")\n",
      "30/78:\n",
      "if((a[0]-b[0])**2 +(a[1]-b[1])**2 < pow(10,-6)):\n",
      "    print(\"Yes\")\n",
      "30/79:\n",
      "if((a[0]-b[0])**2 +(a[1]-b[1])**2 < pow(10,-6)):\n",
      "    print(\"Yes\")\n",
      "30/80:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_new))\n",
      "        if((w_new[0]-w_prev[0])**2 +(w_new[1]-w_prev[1])**2 < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/81: descent([5,10],[5,10],.3, X,y)\n",
      "30/82:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_new))\n",
      "        if((w_new[0]-w_prev[0])**2 < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/83: descent([5,10],[5,10],.3, X,y)\n",
      "30/84:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_new))\n",
      "        if( +(w_new[1]-w_prev[1])**2 < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/85: descent([5,10],[5,10],.3, X,y)\n",
      "30/86:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_new))\n",
      "        if(((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/87: descent([5,10],[5,10],.3, X,y)\n",
      "30/88:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_new))\n",
      "        print((w_new[1]-w_prev[1])**2)\n",
      "        print(w_new[1]-w_prev[1])\n",
      "        if(((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/89: descent([5,10],[5,10],.3, X,y)\n",
      "30/90:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "30/91:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "plt.xlabel(\"X\")\n",
      "plt.ylabel(\"y\")\n",
      "plt.show()\n",
      "30/92:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w, X):\n",
      "    return (w[1]*(np.array(X[:,0]))+w[0])\n",
      "30/93:\n",
      "def cost(w, X, y):\n",
      "    return (np.sum(np.square(LM(w,X)-y)) / (2*m))\n",
      "30/94:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w,X)-y)/m\n",
      "    g[1]= np.multiply(np.sum(LM(w,X)-y),X)/m\n",
      "    return g\n",
      "30/95:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_new))\n",
      "        print((w_new[1]-w_prev[1])**2)\n",
      "        print(w_new[1]-w_prev[1])\n",
      "        if(((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/96: descent([5,10],[5,10],.3, X,y)\n",
      "30/97:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "\n",
      "        if(((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/98: descent([5,10],[5,10],.3, X,y)\n",
      "30/99:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "\n",
      "        if(((w_new[0]-w_prev[0])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/100: descent([5,10],[5,10],.3, X,y)\n",
      "30/101:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print((w_new[1]-w_prev[1])**2)\n",
      "        if(((w_new[0]-w_prev[0])**2 ) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/102: descent([5,10],[5,10],.3, X,y)\n",
      "30/103:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_new))\n",
      "        if(((w_new[0]-w_prev[0])**2 ) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/104: descent([5,10],[5,10],.3, X,y)\n",
      "30/105:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print((w_new[1]-w_prev[1])**2)\n",
      "        if(((w_new[0]-w_prev[0])**2 ) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/106: descent([5,10],[5,10],.3, X,y)\n",
      "30/107:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print((w_new[0]-w_prev[0])**2)\n",
      "        if(((w_new[0]-w_prev[0])**2 ) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/108: descent([5,10],[5,10],.3, X,y)\n",
      "30/109: b\n",
      "30/110:\n",
      "np.sum(a,b)\n",
      "np.sum(a,-b)\n",
      "30/111: np.sum(a,b)\n",
      "30/112: a\n",
      "30/113: b\n",
      "30/114: np.sum(a,b)\n",
      "30/115: type(a[0])\n",
      "30/116: np.sum(a,b)\n",
      "31/9: a=np.array(a)\n",
      "30/117: a=np.array(a)\n",
      "30/118:\n",
      "a=np.array(a)\n",
      "b=np.array(b)\n",
      "30/119: a\n",
      "30/120: np.sum(a,b)\n",
      "30/121:\n",
      "print(type(a))\n",
      "a=np.array(a)[indices.astype(int)]\n",
      "print(type(a))\n",
      "b=np.array(b)\n",
      "print(type(b))\n",
      "30/122:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "        w_prev=np.array(w_prev)\n",
      "        w_new=np.array(w_new)\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "      \n",
      "        if(((np.sum(w_new[0],-w_prev[0]))**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/124:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "      \n",
      "        if(((np.sum(w_new[0],-w_prev[0]))**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/125:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    w_prev=np.array(w_prev)\n",
      "    w_new=np.array(w_new)\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "      \n",
      "        if(((np.sum(w_new[0],-w_prev[0]))**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/126: descent([5,10],[5,10],.3, X,y)\n",
      "30/127: gradient(np.array([5,10]),X,y)\n",
      "30/128: gradient(np.array([5,10]),X,y)[1]\n",
      "30/129: type([0])\n",
      "30/130: type([0]*2)\n",
      "30/131:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "30/132: type(y)\n",
      "30/133:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "30/134: type(y)\n",
      "30/135:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w,X)-np.array(y))/m\n",
      "    g[1]= np.multiply(np.sum(LM(w,X)-np.array(y)),X[:,0])/m\n",
      "    return g\n",
      "30/136:\n",
      "def descent(w_prev, w_new, lr, X, y):\n",
      "        \n",
      "    w_prev=np.array(w_prev)\n",
      "    w_new=np.array(w_new)\n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "      \n",
      "        if(((np.sum(w_new[0],-w_prev[0]))**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/137: descent([5,10],[5,10],.3, X,y)\n",
      "30/138: descent([5,10],[5,10],.3, X,y)\n",
      "30/139:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "      \n",
      "        if(((np.sum(w_new[0],-w_prev[0]))**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/140: descent([5,10],[5,10],.3, X,y)\n",
      "30/141:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "      \n",
      "        if(((w_new[0],-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/142: descent([5,10],[5,10],.3, X,y)\n",
      "30/143:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "      \n",
      "        if(((w_new[0],-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/144: descent([5,10],[5,10],.3, X,y)\n",
      "30/145: pow(10,6)\n",
      "30/146:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        type(w_new)\n",
      "        if(((w_new[0],-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/147: descent([5,10],[5,10],.3, X,y)\n",
      "30/148:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_new))\n",
      "        if(((w_new[0],-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/149: descent([5,10],[5,10],.3, X,y)\n",
      "30/150:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_prev))\n",
      "        if(((w_new[0],-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/151: descent([5,10],[5,10],.3, X,y)\n",
      "30/152:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_prev))\n",
      "        if(((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) < pow(10,-6)):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/153: descent([5,10],[5,10],.3, X,y)\n",
      "30/154:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w,X)-np.array(y))/m\n",
      "    g[1]= np.multiply(np.sum(LM(w,X)-np.array(y)),np.array(X[:,0]))/m\n",
      "    return g\n",
      "30/155: descent([5,10],[5,10],.3, X,y)\n",
      "30/156:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_prev))\n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            break\n",
      "            print(\"break\")\n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/157: descent([5,10],[5,10],.3, X,y)\n",
      "30/158:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_prev))\n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            break            \n",
      "    \n",
      "    return LM(w_new, X)\n",
      "30/159:\n",
      "def cost(w, X, y):\n",
      "    return (np.sum(np.square(LM(w,X)-np.array(y))) / (2*m))\n",
      "32/1:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "32/2:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "plt.xlabel(\"X\")\n",
      "plt.ylabel(\"y\")\n",
      "plt.show()\n",
      "32/3:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w, X):\n",
      "    return (w[1]*(np.array(X[:,0]))+w[0])\n",
      "32/4:\n",
      "def cost(w, X, y):\n",
      "    return (np.sum(np.square(LM(w,X)-np.array(y))) / (2*m))\n",
      "32/5:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w,X)-np.array(y))/m\n",
      "    g[1]= np.multiply(np.sum(LM(w,X)-np.array(y)),np.array(X[:,0]))/m\n",
      "    return g\n",
      "32/6:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                \n",
      "        print(type(w_prev))\n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            break            \n",
      "    \n",
      "    return LM(w_new, X)\n",
      "32/7:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w,X)-np.array(y))/m\n",
      "    g[1]= np.multiply(np.sum(LM(w,X)-np.array(y)),np.array(X[:,0]))/m\n",
      "    return g\n",
      "32/8:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w,X)-np.array(y))/m\n",
      "    g[1]= np.multiply(np.sum(LM(w,X)-np.array(y)),np.array(X[:,0]))/m\n",
      "    return g\n",
      "32/9:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    \n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "        \n",
      "        print(gradient(w_prev,X,y))\n",
      "        \n",
      "        print(type(w_prev))\n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            break            \n",
      "    \n",
      "    return LM(w_new, X)\n",
      "32/10: descent([5,10],[5,10],.3, X,y)\n",
      "32/11:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    c=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "        \n",
      "        print(c++)\n",
      "        \n",
      "        print(type(w_prev))\n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            break            \n",
      "    \n",
      "    return LM(w_new, X)\n",
      "32/12:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    c=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "        \n",
      "        print(c+=1)\n",
      "        \n",
      "        print(type(w_prev))\n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            break            \n",
      "    \n",
      "    return LM(w_new, X)\n",
      "32/13:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    c=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "        \n",
      "        print(c)\n",
      "        c+=1\n",
      "        print(type(w_prev))\n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            break            \n",
      "    \n",
      "    return LM(w_new, X)\n",
      "32/14: descent([5,10],[5,10],.3, X,y)\n",
      "32/15:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w,X)-np.array(y))/m\n",
      "    g[1]= (np.sum((LM(w,X)-np.array(y))*np.array(X[:,0])))/m\n",
      "    return g\n",
      "32/16:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    c=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "        \n",
      "        print(c)\n",
      "        c+=1\n",
      "        print(type(w_prev))\n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            break            \n",
      "    \n",
      "    return LM(w_new, X)\n",
      "32/17: descent([5,10],[5,10],.3, X,y)\n",
      "32/18:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w,X)-np.array(y))/m\n",
      "    g[1]= (np.sum((LM(w,X)-np.array(y))*np.array(X[:,0])))/m\n",
      "    return g\n",
      "32/19:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    c=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "        \n",
      "        print(c)\n",
      "        c+=1\n",
      "        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            break            \n",
      "    \n",
      "    return LM(w_new, X)\n",
      "32/20: descent([5,10],[5,10],.3, X,y)\n",
      "32/21: descent([5,10],[5,10],.1, X,y)\n",
      "32/22: descent([5,10],[5,10],.00001, X,y)\n",
      "32/23: descent([5,10],[5,10],.9, X,y)\n",
      "32/24: descent([5,10],[5,10],.3, X,y)\n",
      "32/25:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    c=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new, X, y))\n",
      "            break            \n",
      "    \n",
      "    return LM(w_new, X)\n",
      "32/26: descent([5,10],[5,10],.3, X,y)\n",
      "32/27:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new, X, y))\n",
      "            return w_new\n",
      "        else if j>500:\n",
      "            return w_new\n",
      "       \n",
      "        j+=1\n",
      "32/28:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new, X, y))\n",
      "            return w_new\n",
      "        else if j>500:\n",
      "            return w_new\n",
      "       \n",
      "        j+=1\n",
      "32/29:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new, X, y))\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new\n",
      "       \n",
      "        j+=1\n",
      "32/30:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new, X, y))\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/31:\n",
      "def descent(w_prev, w_new, lr, X, y):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new, X, y))\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/32:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w):\n",
      "    return (w[1]*(np.array(X[:,0]))+w[0])\n",
      "32/33:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w)-np.array(y))/m\n",
      "    g[1]= (np.sum((LM(w)-np.array(y))*np.array(X[:,0])))/m\n",
      "    return g\n",
      "32/34:\n",
      "def cost(w):\n",
      "    return (np.sum(np.square(LM(w,X)-np.array(y))) / (2*m))\n",
      "32/35:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new, X, y))\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/36:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "32/37:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "plt.xlabel(\"X\")\n",
      "plt.ylabel(\"y\")\n",
      "plt.show()\n",
      "32/38:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w):\n",
      "    return (w[1]*(np.array(X[:,0]))+w[0])\n",
      "32/39:\n",
      "def cost(w):\n",
      "    return (np.sum(np.square(LM(w,X)-np.array(y))) / (2*m))\n",
      "32/40:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w)-np.array(y))/m\n",
      "    g[1]= (np.sum((LM(w)-np.array(y))*np.array(X[:,0])))/m\n",
      "    return g\n",
      "32/41:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev,X,y)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev,X,y)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new, X, y))\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/42: descent([5,10],[5,10],.3, X,y)\n",
      "32/43:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new, X, y))\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/44:\n",
      "def gradient(w,X,y):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w)-np.array(y))/m\n",
      "    g[1]= (np.sum((LM(w)-np.array(y))*np.array(X[:,0])))/m\n",
      "    return g\n",
      "32/45:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new, X, y))\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/46: descent([5,10],[5,10])\n",
      "32/47: descent([5,10],[5,10],.3)\n",
      "32/48:\n",
      "def gradient(w):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w)-np.array(y))/m\n",
      "    g[1]= (np.sum((LM(w)-np.array(y))*np.array(X[:,0])))/m\n",
      "    return g\n",
      "32/49:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new, X, y))\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/50: descent([5,10],[5,10],.3)\n",
      "32/51:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/52: descent([5,10],[5,10],.3)\n",
      "32/53:\n",
      "def cost(w):\n",
      "    return (np.sum(np.square(LM(w)-np.array(y))) / (2*m))\n",
      "32/54: descent([5,10],[5,10],.3)\n",
      "32/55:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/56: descent([5,10],[5,10],.3)\n",
      "32/57:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "32/58:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "plt.xlabel(\"X\")\n",
      "plt.ylabel(\"y\")\n",
      "plt.show()\n",
      "32/59:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w):\n",
      "    return (w[1]*(np.array(X[:,0]))+w[0])\n",
      "32/60:\n",
      "def cost(w):\n",
      "    return (np.sum(np.square(LM(w)-np.array(y))) / (2*m))\n",
      "32/61:\n",
      "def gradient(w):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w)-np.array(y))/m\n",
      "    g[1]= (np.sum((LM(w)-np.array(y))*np.array(X[:,0])))/m\n",
      "    return g\n",
      "32/62:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):\n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/63: descent([5,10],[5,10],.3)\n",
      "32/64:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):            \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/65: descent([5,10],[5,10],.3)\n",
      "32/66:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                            \n",
      "        print(w_new)\n",
      "        print(cost(w_new))\n",
      "        \n",
      "        if (w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2 <= pow(10,-6):            \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/67: descent([5,10],[5,10],.3)\n",
      "32/68: descent([0,-1],[0,1-1],.3)\n",
      "32/69: descent([0,-1],[0,-1],.3)\n",
      "32/70:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                            \n",
      "        print(w_new)\n",
      "        print(cost(w_new))\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):            \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/71: descent([0,-1],[0,-1],.3)\n",
      "32/72:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                            \n",
      "        print(w_new)\n",
      "        print(cost(w_new))\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):            \n",
      "            print((w_new[0]-w_prev[0])**2)\n",
      "            print((w_new[1]-w_prev[1])**2)\n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/73: descent([0,-1],[0,-1],.3)\n",
      "32/74:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                            \n",
      "        print(w_new)\n",
      "        print(cost(w_new))\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):            \n",
      "            print(w_new[0]-w_prev[0])\n",
      "            print(w_new[1]-w_prev[1])\n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/75: descent([0,-1],[0,-1],.3)\n",
      "32/76:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                            \n",
      "        print(w_new)\n",
      "        print(w_prev)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):            \n",
      "            print(w_new[0]-w_prev[0])\n",
      "            print(w_new[1]-w_prev[1])\n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/77: descent([0,-1],[0,-1],.3)\n",
      "32/78:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(lr*gradient(w_prev)[0])\n",
      "        primt(lr*gradient(w_prev)[1])\n",
      "        print(w_new)\n",
      "        print(w_prev)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):            \n",
      "            print(w_new[0]-w_prev[0])\n",
      "            print(w_new[1]-w_prev[1])\n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/79: descent([0,-1],[0,-1],.3)\n",
      "32/80:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(lr*gradient(w_prev)[0])\n",
      "        print(lr*gradient(w_prev)[1])\n",
      "        print(w_new)\n",
      "        print(w_prev)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):            \n",
      "            print(w_new[0]-w_prev[0])\n",
      "            print(w_new[1]-w_prev[1])\n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/81: descent([0,-1],[0,-1],.3)\n",
      "32/82:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(lr*gradient(w_prev)[0])\n",
      "        print(lr*gradient(w_prev)[1])      \n",
      "        print(w_prev)\n",
      "        \n",
      "        print(w_new)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):            \n",
      "            print(w_new[0]-w_prev[0])\n",
      "            print(w_new[1]-w_prev[1])\n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "32/83: descent([0,-1],[0,-1],.3)\n",
      "33/1:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "33/2:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "plt.xlabel(\"X\")\n",
      "plt.ylabel(\"y\")\n",
      "plt.show()\n",
      "33/3:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w):\n",
      "    return (w[1]*(np.array(X[:,0]))+w[0])\n",
      "33/4:\n",
      "def cost(w):\n",
      "    return (np.sum(np.square(LM(w)-np.array(y))) / (2*m))\n",
      "33/5:\n",
      "def gradient(w):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w)-np.array(y))/m\n",
      "    g[1]= (np.sum((LM(w)-np.array(y))*np.array(X[:,0])))/m\n",
      "    return g\n",
      "33/6:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(lr*gradient(w_prev)[0])\n",
      "        print(lr*gradient(w_prev)[1])      \n",
      "        print(w_prev)\n",
      "        \n",
      "        print(w_new)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/7: descent([0,-1],[0,-1],.3)\n",
      "33/8:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(type(lr*gradient(w_prev)[0]))\n",
      "        print(type(lr*gradient(w_prev)[1]))      \n",
      "        print(w_prev)\n",
      "        \n",
      "        print(w_new)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/9: descent([0,-1],[0,-1],.3)\n",
      "33/10:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(type(lr*gradient(w_prev)[0]))\n",
      "        print(type(lr*gradient(w_prev)[1]))      \n",
      "        print(type(w_prev[0]))\n",
      "        \n",
      "        print(w_new)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/11: descent([0,-1],[0,-1],.3)\n",
      "33/12:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(type(lr*gradient(w_prev)[0]))\n",
      "        print(type(lr*gradient(w_prev)[1]))      \n",
      "        print(w_prev[0])\n",
      "        \n",
      "        print(w_new)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/13: descent([0,-1],[0,-1],.3)\n",
      "33/14:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(type(lr*gradient(w_prev)[0]))\n",
      "        print(type(lr*gradient(w_prev)[1]))      \n",
      "        print(w_prev)\n",
      "        \n",
      "        print(w_new)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/15: descent([0,-1],[0,-1],.3)\n",
      "33/16:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev[0])\n",
      "        print(lr*gradient(w_prev)[0])\n",
      "        print(w_prev[0]- lr*gradient(w_prev)[0])\n",
      "        \n",
      "        print(\"\\n\",w_prev)        \n",
      "        print(w_new)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/17: descent([0,-1],[0,-1],.3)\n",
      "33/18:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev[0])\n",
      "        print(lr*gradient(w_prev)[0])\n",
      "        print(type(w_prev[0]- lr*gradient(w_prev)[0]))\n",
      "        \n",
      "        print(\"\\n\",w_prev)        \n",
      "        print(w_new)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/19: descent([0,-1],[0,-1],.3)\n",
      "33/20:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev[0])\n",
      "        print(lr*gradient(w_prev)[0])\n",
      "        print(w_prev[0]- lr*gradient(w_prev)[0])\n",
      "        \n",
      "        print(\"\\n\",w_prev)        \n",
      "        print(w_new[0])\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/21: descent([0,-1],[0,-1],.3)\n",
      "33/22:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev[0])\n",
      "        print(lr*gradient(w_prev)[0])\n",
      "        print(w_prev[0]- lr*gradient(w_prev)[0])\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        print(w_new[0])\n",
      "        \n",
      "        print(\"\\n\",w_prev)        \n",
      "        print(w_new[0])\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/23: descent([0,-1],[0,-1],.3)\n",
      "33/24:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev[0])\n",
      "        print(lr*gradient(w_prev)[0])\n",
      "        print(w_prev[0]- lr*gradient(w_prev)[0])\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        print(w_new[0])\n",
      "        \n",
      "        print(\"\\n\",w_prev)        \n",
      "        print(w_new)\n",
      "        \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/25: descent([0,-1],[0,-1],.3)\n",
      "33/26:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev)\n",
      "        print(w_new)\n",
      "       \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/27:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev)\n",
      "        print(w_new)\n",
      "       \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/28: descent([0,-1],[0,-1],.3)\n",
      "33/29:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev)\n",
      "        print(w_new)\n",
      "        print(w_new[0], w_new[1])\n",
      "       \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "33/30: descent([0,-1],[0,-1],.3)\n",
      "34/1:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "34/2:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "plt.xlabel(\"X\")\n",
      "plt.ylabel(\"y\")\n",
      "plt.show()\n",
      "34/3:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w):\n",
      "    return (w[1]*(np.array(X[:,0]))+w[0])\n",
      "34/4:\n",
      "def cost(w):\n",
      "    return (np.sum(np.square(LM(w)-np.array(y))) / (2*m))\n",
      "34/5:\n",
      "def gradient(w):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w)-np.array(y))/m\n",
      "    g[1]= (np.sum((LM(w)-np.array(y))*np.array(X[:,0])))/m\n",
      "    return g\n",
      "34/6:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev)\n",
      "        print(w_new)\n",
      "        print(w_new[0], w_new[1])\n",
      "       \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "34/7: descent([0,-1],[0,-1],.3)\n",
      "34/8:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        \n",
      "        ## Why not getting saved?!!!!!!\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev)\n",
      "        print(w_new)\n",
      "        print(w_new[0], w_new[1])\n",
      "       \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "34/9:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        \n",
      "        ## Why not getting saved?!!!!!!\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev)\n",
      "        print(w_new)\n",
      "        print(lr*gradient(w_prev)[0])\n",
      "       \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "34/10: descent([0,-1],[0,-1],.3)\n",
      "37/1:\n",
      "# generating fake data\n",
      "from sklearn.datasets import make_regression\n",
      "X,y= make_regression(n_samples=400, n_features=1, n_informative=1, noise=6, bias=30)\n",
      "m=200\n",
      "37/2:\n",
      "import matplotlib.pyplot as plt\n",
      "plt.scatter(X, y, c=\"red\", alpha=0.5, marker=\"o\")\n",
      "plt.xlabel(\"X\")\n",
      "plt.ylabel(\"y\")\n",
      "plt.show()\n",
      "37/3:\n",
      "import numpy as np\n",
      "\n",
      "def LM(w):\n",
      "    return (w[1]*(np.array(X[:,0]))+w[0])\n",
      "37/4:\n",
      "def cost(w):\n",
      "    return (np.sum(np.square(LM(w)-np.array(y))) / (2*m))\n",
      "37/5:\n",
      "def gradient(w):\n",
      "    g=[0]*2 # This is how to initialise arrays.\n",
      "    g[0]= np.sum(LM(w)-np.array(y))/m\n",
      "    g[1]= (np.sum((LM(w)-np.array(y))*np.array(X[:,0])))/m\n",
      "    return g\n",
      "37/6:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        w_prev=w_new\n",
      "        \n",
      "        ## Why not getting saved?!!!!!!\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev)\n",
      "        print(w_new)\n",
      "        print(lr*gradient(w_prev)[0])\n",
      "       \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/7: descent([0,-1],[0,-1],.3)\n",
      "37/8:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        \n",
      "        w_prev=w_new        \n",
      "        ## Why not getting saved?!!!!!!\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev)\n",
      "        print(w_new)\n",
      "        print(lr*gradient(w_prev)[0])\n",
      "       \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "37/9:\n",
      "def descent(w_prev, w_new, lr):         \n",
      "    j=0\n",
      "    while True:\n",
      "        \n",
      "        w_prev=w_new        \n",
      "        ## Why not getting saved?!!!!!!\n",
      "        w_new[0]=w_prev[0]- lr*gradient(w_prev)[0]\n",
      "        w_new[1]=w_prev[1]- lr*gradient(w_prev)[1]\n",
      "                                \n",
      "        print(w_prev)\n",
      "        print(w_new)\n",
      "        print(lr*gradient(w_prev)[0])\n",
      "       \n",
      "        if ((w_new[0]-w_prev[0])**2 + (w_new[1]-w_prev[1])**2) <= pow(10,-6):                        \n",
      "            #print(\"The lowest value of cost function is\",cost(w_new))\n",
      "            #print(\"And the corresponding weights are\",w_new)\n",
      "            return w_new\n",
      "        elif j>500:\n",
      "            return w_new       \n",
      "        j+=1\n",
      "37/10: descent([0,-1],[0,-1],.3)\n",
      "37/11: descent([0,-1],[1,2],.3)\n",
      "   1: history\n",
      "   2: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
